package com.example.mcpclient

import io.ktor.client.*
import io.ktor.client.call.*
import io.ktor.client.engine.cio.*
import io.ktor.client.plugins.*
import io.ktor.client.plugins.contentnegotiation.*
import io.ktor.client.request.*
import io.ktor.http.*
import io.ktor.serialization.kotlinx.json.*
import io.modelcontextprotocol.kotlin.sdk.*
import io.modelcontextprotocol.kotlin.sdk.client.Client
import io.modelcontextprotocol.kotlin.sdk.client.StdioClientTransport
import kotlinx.coroutines.runBlocking
import kotlinx.io.asSink
import kotlinx.io.asSource
import kotlinx.io.buffered
import kotlinx.serialization.Serializable
import kotlinx.serialization.SerializationException
import kotlinx.serialization.json.*
import org.slf4j.LoggerFactory
import java.io.File
import java.io.IOException
import kotlin.system.exitProcess

// --- Configuration & Globals ---

// Retrieve API key from environment variable, exit if not found.
val apiKey: String = System.getenv("LLM_API_KEY") ?: run {
  System.err.println("Error: LLM_API_KEY not found in environment variables.")
  exitProcess(1)
}

// Shared JSON configuration for serialization/deserialization.
val json = Json {
  ignoreUnknownKeys = true // Be lenient with unexpected fields from the API.
  prettyPrint = true       // Useful for debugging JSON payloads.
  encodeDefaults = true    // Ensure all fields are present in serialized output.
}

// --- LLM Data Structures ---

@Serializable
data class LLMMessage(val role: String, val content: String)

@Serializable
data class LLMRequest(
  val messages: List<LLMMessage>,
  val model: String = "llama3-70b-8192", // Default model
  val temperature: Double = 0.1,         // Controls randomness (lower = more deterministic)
  val max_tokens: Int = 1024,            // Max response length
  val stream: Boolean = false            // Streaming is not used in this example
)

@Serializable
data class LLMChoice(val message: LLMMessage)

@Serializable
data class LLMResponse(val choices: List<LLMChoice>) {
  // Helper to easily extract the content of the first message.
  fun firstMessageContent(): String? = choices.firstOrNull()?.message?.content
}

// Represents the expected JSON structure when the LLM decides to call a tool.
@Serializable
data class LLMToolCall(
  val tool_name: String,
  val arguments: JsonObject
)

// Represents the parsed outcome of an LLM response: either plain text or a tool call request.
sealed class ParsedLLMResponse {
  data class TextResponse(val text: String) : ParsedLLMResponse()
  data class ToolCallResponse(val toolCall: LLMToolCall) : ParsedLLMResponse()
}

// --- LLM API Client ---

class LLMClient(apiKey: String) : AutoCloseable {
  private val logger = LoggerFactory.getLogger(LLMClient::class.java)

  // Configured Ktor HTTP client for interacting with the LLM API.
  private val client = HttpClient(CIO) {
    install(ContentNegotiation) { // Enable JSON serialization/deserialization
      json(json)
    }
    defaultRequest { // Apply common headers to all requests
      headers {
        append(HttpHeaders.Authorization, "Bearer $apiKey")
        append(HttpHeaders.ContentType, ContentType.Application.Json)
      }
    }
    install(HttpTimeout) { // Set reasonable timeouts
      requestTimeoutMillis = 60_000 // 60 seconds
    }
    // Optional: Enable detailed Ktor request/response logging during development
    // install(Logging) { level = LogLevel.INFO }
  }

  /**
   * Sends messages to the LLM API and parses the response.
   * @return ParsedLLMResponse indicating either a direct text answer or a tool call request.
   * @throws IOException if the API request fails or the response is invalid.
   */
  suspend fun getResponse(messages: List<LLMMessage>): ParsedLLMResponse {
    val url = "https://api.groq.com/openai/v1/chat/completions" // Target LLM API endpoint
    val payload = LLMRequest(messages = messages)

    logger.debug("Sending LLM request to {}", url)

    try {
      val response: LLMResponse = client.post(url) {
        setBody(payload)
      }.body() // Execute POST request and deserialize JSON response

      logger.debug("Received LLM response.")

      val content = response.firstMessageContent() ?: run {
        logger.error("LLM response content is missing or invalid. Response: {}", response)
        throw IOException("Invalid LLM response format: missing content.")
      }

      // Attempt to parse the content as a ToolCall JSON.
      // If parsing fails (SerializationException or invalid structure), treat it as plain text.
      return try {
        // Trim whitespace which might interfere with JSON parsing
        val toolCall = json.decodeFromString<LLMToolCall>(content.trim())
        ParsedLLMResponse.ToolCallResponse(toolCall)
      } catch (e: SerializationException) {
        // Expected case for non-tool-call responses
        ParsedLLMResponse.TextResponse(content)
      } catch (e: IllegalArgumentException) {
        // Catch cases where it's JSON but not the expected ToolCall structure
        logger.warn("Could not parse content as ToolCall JSON, treating as text: {}", content)
        ParsedLLMResponse.TextResponse(content)
      }

    } catch (e: ClientRequestException) {
      // Handle HTTP errors (4xx, 5xx)
      val errorBody = runCatching { e.response.body<String>() }.getOrDefault("(could not read error body)")
      logger.error("LLM HTTP Client Error: {} - Response Body: {}", e.response.status, errorBody, e)
      throw IOException("LLM API request failed with status ${e.response.status}", e)
    } catch (e: Exception) {
      // Handle other potential errors (network issues, unexpected exceptions)
      logger.error("Error during LLM API call: {}", e.message, e)
      throw IOException("Generic error during LLM call", e)
    }
  }

  // Closes the underlying Ktor HTTP client.
  override fun close() {
    client.close()
    logger.info("LLM Client closed.")
  }
}

// --- Tool Formatting Utility ---

/**
 * Formats the list of available MCP tools into a string suitable for an LLM system prompt.
 * Describes the tools, their parameters, and how the LLM should format a tool call request.
 */
fun formatToolsForLlm(toolsResult: ListToolsResult?): String {
  val logger = LoggerFactory.getLogger("ToolFormatter")
  val sb = StringBuilder("You have access to the following tools. Use them when appropriate:\n\n")
  val actualTools = toolsResult?.tools ?: emptyList()

  if (actualTools.isEmpty()) {
    sb.append("No tools are available.\n")
  } else {
    actualTools.forEach { tool ->
      val toolName = tool.name
      val toolDesc = tool.description?.replace("\\s+".toRegex(), " ")?.trim() ?: "No description provided."
      val schema = tool.inputSchema
      val properties = schema?.properties ?: JsonObject(emptyMap())
      val requiredParams = schema?.required ?: emptyList()

      sb.append("## Tool: `${toolName}`\n") // Use Markdown for clarity
      sb.append("**Description:** $toolDesc\n")

      if (properties.isEmpty()) {
        sb.append("**Parameters:** None\n")
      } else {
        sb.append("**Parameters:**\n")
        properties.forEach { (name, detailsJsonElement) ->
          // Extract type information, default to 'any' if parsing fails
          val type = (detailsJsonElement as? JsonObject)?.get("type")?.jsonPrimitive?.content ?: "any"
          val description = (detailsJsonElement as? JsonObject)?.get("description")?.jsonPrimitive?.content
          val isRequired = name in requiredParams

          sb.append("  - `$name` ($type)")
          if (isRequired) sb.append(" (required)")
          if (description != null) sb.append(": $description")
          sb.append("\n")
        }
      }
      sb.append("---\n") // Separator between tools
    }
  }

  // Crucial instructions for the LLM on how to request a tool call
  sb.append("\n**How to Use Tools:**\n")
  sb.append("If you need to use a tool, respond ONLY with a single JSON object like this:\n")
  sb.append("```json\n")
  sb.append("{\n")
  sb.append("  \"tool_name\": \"<name_of_tool_to_use>\",\n")
  sb.append("  \"arguments\": { \"<param_name>\": \"<param_value>\", ... }\n")
  sb.append("}\n")
  sb.append("```\n")
  sb.append("Make sure the `arguments` object contains all required parameters for the chosen tool.\n")
  sb.append("If you do not need to use a tool, respond with your answer directly as plain text.\n")

  val formattedPrompt = sb.toString()
  logger.debug("Formatted tools prompt for LLM:\n{}", formattedPrompt) // Log the full prompt at debug level
  return formattedPrompt
}

// --- Main Application Logic ---

fun main(args: Array<String>) = runBlocking {
  val logger = LoggerFactory.getLogger("Main")
  logger.info("Starting Kotlin MCP Client Example...")

  // Determine the path to the server script (allow override via args)
  val serverScriptPath = args.firstOrNull() ?: "server.main.kts" // Default path
  val serverScriptFile = File(serverScriptPath).absoluteFile
  if (!serverScriptFile.exists() || !serverScriptFile.isFile) {
    logger.error("Error: Server script not found or is not a file at '{}'", serverScriptFile.path)
    exitProcess(1)
  }
  logger.info("Using server script: {}", serverScriptFile.path)

  // Initialize clients within a try block to ensure cleanup
  var llmClient: LLMClient? = null
  var mcpClient: Client? = null
  var serverProcess: Process? = null

  try {
    // Initialize LLM client (can be done outside process scope)
    llmClient = LLMClient(apiKey)

    // Initialize MCP client information
    mcpClient = Client(clientInfo = Implementation(name = "kotlin-mcp-client-example", version = "1.0.0"))

    // Start the MCP server script as a separate process
    logger.info("Starting MCP server process: kotlin {}", serverScriptFile.path)
    val command = listOf("kotlin", serverScriptFile.path) // Assuming 'kotlin' command runs .kts files
    serverProcess = ProcessBuilder(command)
//      .redirectErrorStream(true) // Redirect server's stderr to this process's input stream (for logging)
      .start()
    logger.info("Server process started (PID: {}). Waiting for MCP connection...", serverProcess.pid())

    // Connect to the server process via standard input/output
    val transport = StdioClientTransport(
      input = serverProcess.inputStream.asSource().buffered(), // Read from server's stdout
      output = serverProcess.outputStream.asSink().buffered()  // Write to server's stdin
    )
    mcpClient.connect(transport)
    logger.info("MCP connection established with server.")

    // Get the list of tools from the connected MCP server
    logger.info("Listing available tools from MCP server...")
    val toolsResponse = mcpClient.listTools()
    val toolsPrompt = formatToolsForLlm(toolsResponse)
    logger.info("Tool information retrieved. {} tools available.", toolsResponse?.tools?.size ?: 0)

    // --- Core Interaction Loop ---
    val userQuestion = "What is the weather in London?" // Example user query
    logger.info("User question: '{}'", userQuestion)

    val messages = mutableListOf(
      LLMMessage("system", toolsPrompt), // Provide tool context to the LLM
      LLMMessage("user", userQuestion)
    )

    logger.info("Sending initial request to LLM...")
    val llmResponse = llmClient.getResponse(messages)

    var finalResponseText: String

    when (llmResponse) {
      is ParsedLLMResponse.TextResponse -> {
        // LLM answered directly without needing a tool
        logger.info("LLM provided a direct text response.")
        finalResponseText = llmResponse.text
      }
      is ParsedLLMResponse.ToolCallResponse -> {
        // LLM requested a tool call
        val toolCall = llmResponse.toolCall
        logger.info("LLM requested tool call: Name='{}', Args='{}'", toolCall.tool_name, toolCall.arguments)

        // Add the LLM's request to the message history (as 'assistant')
        messages.add(LLMMessage("assistant", json.encodeToString(LLMToolCall.serializer(), toolCall)))

        // Execute the tool call via MCP
        logger.info("Executing tool '{}' via MCP...", toolCall.tool_name)
        val toolResult: CallToolResultBase? = mcpClient.callTool(toolCall.tool_name, toolCall.arguments)

        // Process the tool result (handle potential nulls or different content types)
        val toolResultText = toolResult?.content?.joinToString("\n") { content ->
          when (content) {
            is TextContent -> content.text ?: "[Empty TextContent]"
            else -> "[Received non-text content: ${content::class.simpleName}]"
          }
        } ?: "[No content received from tool execution]"
        logger.info("Tool '{}' executed. Result: '{}'", toolCall.tool_name, toolResultText)

        // Add the tool result to the message history (as 'user' or 'tool' role - 'user' is common)
        // Provide clear context about which tool was run and what it returned.
        messages.add(
          LLMMessage(
            "user", // Or use "tool" role if supported and preferred by the LLM
            "The tool '${toolCall.tool_name}' was executed and returned the following result:\n$toolResultText\n\nPlease use this information to answer the original question."
          )
        )

        // Ask the LLM again, now with the tool result in context
        logger.info("Sending follow-up request to LLM with tool results...")
        val finalLlmResponse = llmClient.getResponse(messages)

        // Get the final text answer from the second LLM response
        finalResponseText = when (finalLlmResponse) {
          is ParsedLLMResponse.TextResponse -> finalLlmResponse.text
          is ParsedLLMResponse.ToolCallResponse -> {
            // LLM responded with *another* tool call, which is unusual here.
            // Log a warning and provide the raw JSON as the 'final' answer for debugging.
            logger.warn("LLM unexpectedly requested another tool call after receiving tool results. Raw response: {}", finalLlmResponse.toolCall)
            "LLM requested another tool call unexpectedly: ${json.encodeToString(LLMToolCall.serializer(), finalLlmResponse.toolCall)}"
          }
        }
      }
    }

    // Output the final result
    logger.info("Received final response from LLM.")
    println("\n=== Final Answer ===")
    println(finalResponseText)
    println("====================")

  } catch (e: Exception) {
    // Catch any unexpected errors during the main execution
    logger.error("\n--- An unhandled error occurred in the main client logic ---", e)
    System.err.println("\nClient Runtime Error: ${e::class.simpleName} - ${e.message}")
    // Consider exiting with a non-zero code on failure
    // exitProcess(2)
  } finally {
    // --- Cleanup ---
    logger.info("\nInitiating cleanup...")

    // Close MCP Client
    try {
      mcpClient?.close()
      logger.info("MCP Client closed successfully.")
    } catch (e: Exception) {
      logger.error("Error closing MCP client", e)
    }

    // Close LLM Client
    try {
      llmClient?.close()
      // Logger message is inside LLMClient.close()
    } catch (e: Exception) {
      logger.error("Error closing LLM client", e)
    }

    // Terminate Server Process
    serverProcess?.let { proc ->
      if (proc.isAlive) {
        logger.info("Attempting to terminate MCP server process (PID: {})...", proc.pid())
        proc.destroy() // Request graceful termination first
        val terminatedGracefully = proc.waitFor(5, java.util.concurrent.TimeUnit.SECONDS) // Wait briefly

        if (!terminatedGracefully && proc.isAlive) {
          logger.warn("Server process did not terminate gracefully, forcing termination.")
          proc.destroyForcibly() // Force termination if needed
        }

        val exitCode = proc.waitFor() // Wait for process to fully exit
        logger.info("MCP server process terminated with exit code: {}.", exitCode)
      } else {
        // Process already finished, just log its exit code if possible
        val exitCode = try { proc.exitValue() } catch (_: IllegalThreadStateException) { "N/A (already exited)" }
        logger.info("MCP server process had already terminated with exit code: {}.", exitCode)
      }
    }
    logger.info("Client finished execution.")
  }
}